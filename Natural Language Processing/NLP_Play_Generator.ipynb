{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Play Generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Play Generator"
      ],
      "metadata": {
        "id": "OHREgBkiZ9Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task is to build a model to predict the next character in a sequence.\n",
        "\n",
        "What we will do is to train the model on some sequences of the play Romeo and Jolliet, and we take the output which is the next character, and keep feeding the output with the previous input to the model as much as we want to predict a whole play eventually."
      ],
      "metadata": {
        "id": "pjnIiEvoaAuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8UcjcInBYy5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the dataset"
      ],
      "metadata": {
        "id": "OJdC4LL2bBD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F1iYmRbYzY1",
        "outputId": "9e8d5f8e-a505-4ba7-989d-7ad6cc3d7cb1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XeOU9VYY2GU",
        "outputId": "e0819a0e-c239-4808-bd2c-9db8efe8ee07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eQX-xysY3Fs",
        "outputId": "a6f9f37f-7198-4671-926e-4e1a7359b3fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "BmhumrL-Y4Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is not processed earlier, we need to preprocess it."
      ],
      "metadata": {
        "id": "5l09_RXgdQZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text)) # Sort all the unique characters in the text\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)} # the output: letter:index\n",
        "idx2char = np.array(vocab) # reverse mapping, index:character\n",
        "\n",
        "# Defining a function to encode the text: convert to integer representation\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "LlWiRrePY5f1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPdjDizIY6Ci",
        "outputId": "39b48aa0-41a0-45ca-bf77-8693c614235b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding"
      ],
      "metadata": {
        "id": "x39OI7nIevT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  # convert it to numpy array if it is not one already\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpGm6h2ZY7ek",
        "outputId": "86fa3682-92f8-43f6-b8bb-a7b35aa56fec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating training examples"
      ],
      "metadata": {
        "id": "tk8lSw2EY9rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is not feazible to pass all the data to our model at once, we need to pass something meaningfull.\n",
        "\n",
        "We will pass a sequence of a length (100) to the model, and the output will be the same sequence shifted one letter to the right.\n",
        "\n",
        "*Example:* Input:Hell, Output:ello\n",
        "\n",
        "The output is the input without the first charecter in addition to the last charecter (which is the predection) "
      ],
      "metadata": {
        "id": "U4J7QWvKfRyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  #the length of each batch # length of sequence for a training example\n",
        "# for eveery training example we need to creat a 100 sequence long as an input \n",
        "# and a 100 sequence long as an output. Thus there is 101 character are used for every \n",
        "# training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)  \n",
        "\n",
        "# Create training examples / targets\n",
        "# Converting the text we have to charecters\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "eh-pA4HzY_yg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the batches for the training \n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "NQ3hvtoMZEU1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the sequences into train and test dataset\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "4VjgDW9mZFJU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytkILy_nZF-9",
        "outputId": "c680c574-bd71-4c42-951e-d7f0ec28d4dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Data is Batched and Shuffled\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "vPoQSVAEZG08"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "-AZLINVRZHrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to creat the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]), \n",
        "                               # None here refers to the length of each entry (in the training data we know that each entry has a length of 64, but we do not know in the prediction)\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        # the above line means to return the intermediate step, \n",
        "                        # if it was False then the model will return what the model found on the last step only\n",
        "                        # But we want the output of each step\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'), # glorot_uniform: a good default to start with\n",
        "    tf.keras.layers.Dense(vocab_size) #output, number of nodes = number of vocabs, contain the probability for each charecter\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "# we build the model with 64 Batch size which means that it takes 64 training example and gives a 64 output\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFxOtjpVZJGo",
        "outputId": "136f3767-73ca-42f4-ff35-54ebea80c802"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function"
      ],
      "metadata": {
        "id": "3OXOfEU8ZKbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying the model without training on an example of the training data"
      ],
      "metadata": {
        "id": "rPjqA_JYm1k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j09mAyhZLiD",
        "outputId": "f799ecbc-6539-4aca-9267-9eec17b4fc9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIZXdsuEZMyM",
        "outputId": "e6da151e-01e3-4aae-926d-72b3b6107835"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 4.20983182e-03  8.02002288e-03 -4.81667346e-04 ... -3.38387676e-03\n",
            "   -6.78760058e-04  4.20863414e-03]\n",
            "  [-3.42880696e-04  1.03519168e-02 -6.09154766e-03 ...  1.52908126e-03\n",
            "    3.00851557e-03  7.17437733e-03]\n",
            "  [ 4.40114969e-03  8.14041123e-03 -6.80756150e-03 ...  3.58796376e-03\n",
            "    4.90041124e-03  2.43758271e-03]\n",
            "  ...\n",
            "  [ 6.23601722e-03  6.69120764e-03 -2.35187399e-05 ...  9.47748590e-03\n",
            "    5.31150587e-03 -1.73910556e-03]\n",
            "  [ 2.00737570e-03  3.78023344e-03  8.57006852e-03 ...  7.02480972e-03\n",
            "   -3.66263418e-03 -6.08335808e-03]\n",
            "  [ 4.57030442e-03  3.75389750e-03  6.86121685e-03 ...  3.13449861e-03\n",
            "   -1.88101374e-03 -9.51782917e-04]]\n",
            "\n",
            " [[-2.32595345e-03  9.12027142e-04  9.17745114e-04 ... -5.35471551e-03\n",
            "    5.01336763e-03  2.92073772e-03]\n",
            "  [ 2.75188126e-03  6.12949952e-03 -4.08854662e-03 ... -1.53548535e-05\n",
            "    2.80959811e-03  4.04890088e-05]\n",
            "  [ 2.93527450e-03 -8.84137291e-04 -5.94140089e-04 ... -4.25737031e-04\n",
            "    3.45557788e-03 -4.42400668e-03]\n",
            "  ...\n",
            "  [-2.59005162e-03 -5.54654468e-03 -3.05141788e-03 ...  1.19117182e-02\n",
            "   -7.45549682e-04 -1.03407139e-02]\n",
            "  [-9.38851736e-04 -3.09532834e-03 -2.23190826e-03 ...  8.73764418e-03\n",
            "    2.26421384e-04 -6.01353450e-03]\n",
            "  [ 3.06364801e-03  6.02798490e-03 -2.23367312e-03 ...  4.44261404e-03\n",
            "   -5.24144038e-04 -1.49359345e-03]]\n",
            "\n",
            " [[-1.18759265e-02  2.10953085e-03  4.44251392e-03 ... -5.82003593e-03\n",
            "   -7.83719774e-03  1.00673307e-02]\n",
            "  [-3.90698621e-03  1.02682188e-02  2.87787430e-03 ... -7.71804247e-03\n",
            "   -6.50606956e-03  1.13945510e-02]\n",
            "  [-8.43865419e-05  7.11522391e-03  6.64444291e-04 ... -3.81055404e-03\n",
            "   -2.47153174e-03  6.11393619e-03]\n",
            "  ...\n",
            "  [ 1.54342607e-03  4.43120394e-03 -2.33188714e-03 ...  7.83129130e-03\n",
            "    1.92894216e-03  3.68910329e-03]\n",
            "  [ 5.78653999e-03 -9.86028244e-05 -6.15213707e-04 ...  6.95582107e-03\n",
            "    7.07971072e-03 -2.75268266e-03]\n",
            "  [ 5.29955467e-03 -3.50293028e-03 -4.54016775e-03 ...  8.97874590e-03\n",
            "    4.75688744e-03 -9.02279001e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.71134002e-03  6.17604749e-03 -4.54125693e-03 ...  3.63235199e-03\n",
            "   -1.43477228e-03 -1.86015666e-03]\n",
            "  [ 1.33054366e-03 -1.41932454e-04 -4.68293019e-03 ...  2.99027306e-03\n",
            "   -2.65096663e-03 -2.72248231e-04]\n",
            "  [ 3.90616199e-03 -6.13257231e-04 -5.52211422e-03 ...  4.53253277e-03\n",
            "    5.89768577e-04 -3.45407100e-03]\n",
            "  ...\n",
            "  [ 4.80215391e-03 -4.64574061e-03 -6.77786767e-03 ...  1.40033904e-02\n",
            "    3.62459850e-03 -9.73072834e-03]\n",
            "  [ 4.57931124e-03 -5.13156736e-03 -4.29404667e-03 ...  8.58139619e-03\n",
            "    3.14932503e-03 -3.80636961e-03]\n",
            "  [ 4.43481561e-03 -6.01473963e-03 -4.97232424e-03 ...  5.53749222e-03\n",
            "    4.27690940e-03 -3.81996226e-03]]\n",
            "\n",
            " [[ 1.33365567e-03  9.33734816e-04  4.56630863e-04 ... -3.62718781e-03\n",
            "   -9.92849586e-04 -2.37702415e-03]\n",
            "  [ 5.21001406e-03  8.65676720e-03  3.84572049e-05 ... -7.14305602e-03\n",
            "   -4.64375131e-04  2.39044102e-03]\n",
            "  [ 5.31664491e-03  6.65347790e-03  1.01414334e-04 ... -6.89330325e-03\n",
            "    1.75461860e-03  5.34286210e-03]\n",
            "  ...\n",
            "  [ 1.45069731e-03  1.71848130e-03 -2.42784596e-03 ...  9.13796201e-03\n",
            "    7.64407357e-03 -6.55095698e-03]\n",
            "  [ 2.39479402e-03  2.16369121e-03 -1.27634068e-03 ...  4.85105906e-03\n",
            "    5.85857034e-03 -1.33030047e-03]\n",
            "  [-1.86459173e-03 -2.32448583e-04  7.62318727e-03 ...  3.89888790e-03\n",
            "   -2.70062662e-03 -5.37806470e-03]]\n",
            "\n",
            " [[-7.42207922e-04 -7.05009792e-03 -1.92018924e-03 ...  4.76249354e-03\n",
            "   -3.14379507e-03 -4.12766938e-04]\n",
            "  [ 6.87684631e-04 -5.64792100e-03 -2.40045134e-03 ... -8.32672289e-04\n",
            "    1.46640989e-03 -2.87953578e-03]\n",
            "  [ 2.00545345e-03 -4.35210718e-03 -2.74023740e-04 ... -2.28928961e-03\n",
            "    2.33926717e-03  1.59977842e-03]\n",
            "  ...\n",
            "  [ 5.15946606e-03  5.88444993e-03  1.24382507e-03 ...  3.15577630e-03\n",
            "    5.51655714e-04 -1.12379785e-03]\n",
            "  [ 4.75608418e-03  2.12494773e-03 -4.25608829e-03 ...  7.63469795e-03\n",
            "    1.84247142e-03 -8.20821431e-03]\n",
            "  [ 7.09800562e-03  7.91219436e-03 -8.05651490e-03 ...  8.74890666e-03\n",
            "    4.41556593e-04 -8.13888758e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction at all the timesteps\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9QmVFrmZNfd",
        "outputId": "fc6ed39d-d1e6-410b-e4ab-d68076089b07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 4.2098318e-03  8.0200229e-03 -4.8166735e-04 ... -3.3838768e-03\n",
            "  -6.7876006e-04  4.2086341e-03]\n",
            " [-3.4288070e-04  1.0351917e-02 -6.0915477e-03 ...  1.5290813e-03\n",
            "   3.0085156e-03  7.1743773e-03]\n",
            " [ 4.4011497e-03  8.1404112e-03 -6.8075615e-03 ...  3.5879638e-03\n",
            "   4.9004112e-03  2.4375827e-03]\n",
            " ...\n",
            " [ 6.2360172e-03  6.6912076e-03 -2.3518740e-05 ...  9.4774859e-03\n",
            "   5.3115059e-03 -1.7391056e-03]\n",
            " [ 2.0073757e-03  3.7802334e-03  8.5700685e-03 ...  7.0248097e-03\n",
            "  -3.6626342e-03 -6.0833581e-03]\n",
            " [ 4.5703044e-03  3.7538975e-03  6.8612169e-03 ...  3.1344986e-03\n",
            "  -1.8810137e-03 -9.5178292e-04]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For every single training example we get an output with the same length of the trainig example."
      ],
      "metadata": {
        "id": "rUIGnkUMnOn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally we'll look at a prediction at the first timestep\n",
        "# it contains the propapility of the occurence of every charecter at the first time step.\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8BQWQ7UZO0V",
        "outputId": "416430da-c82c-46ba-ea34-8152d52d0346"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 0.00420983  0.00802002 -0.00048167 -0.00579414  0.00295614  0.00603179\n",
            " -0.00179811 -0.00146926  0.0032317   0.00214365 -0.00529083 -0.0031162\n",
            " -0.00213845  0.00276268  0.007439    0.00456556  0.00602576  0.00366999\n",
            "  0.00468932 -0.00479632 -0.00343378  0.00349788 -0.00104607 -0.00250326\n",
            "  0.00560063 -0.00284219 -0.00445524 -0.00041743  0.00503827 -0.00266773\n",
            "  0.00115404  0.0003496   0.00012188  0.00230692  0.00024695 -0.00245997\n",
            " -0.00106231  0.00094773 -0.00039294  0.00021231  0.00286791 -0.00435733\n",
            "  0.0013087   0.00200386  0.00222673 -0.00573003 -0.00290429  0.0002569\n",
            "  0.00172754  0.00458243 -0.00210922 -0.00088042  0.00163365  0.00128896\n",
            " -0.00465017 -0.00178163  0.00445363  0.00352368  0.0051429   0.00147067\n",
            " -0.0022966  -0.00439372 -0.00338388 -0.00067876  0.00420863], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "# sampling is picking a value based on the propapility distribution, and it does not guarante that we've picked the greatest probability\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "id": "QiS0frRfZPfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "6nGThIRVZQi0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the model"
      ],
      "metadata": {
        "id": "AuJwEZ3JZRdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "2nnxFvCqZTNW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Checkpoint"
      ],
      "metadata": {
        "id": "vPydxBDrZUQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "-hsqlDsiZWN6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "MinyhC-AZXg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry2KoFpKZYa8",
        "outputId": "8fd93432-9413-4a4f-8493-6087b6cb511e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 32s 157ms/step - loss: 2.6237\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 30s 158ms/step - loss: 1.9472\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 1.6958\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 1.5498\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 1.4608\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 1.3997\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 1.3540\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.3154\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.2812\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.2485\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 1.2172\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 1.1860\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 1.1530\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 1.1195\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 31s 164ms/step - loss: 1.0847\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 30s 160ms/step - loss: 1.0479\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 1.0094\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.9713\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.9330\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.8941\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.8564\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.8194\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.7849\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.7523\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.7213\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.6925\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.6675\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.6412\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.6203\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.5995\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.5806\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.5646\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.5495\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.5352\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.5235\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.5125\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.5020\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.4926\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 29s 160ms/step - loss: 0.4845\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 30s 163ms/step - loss: 0.4764\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.4700\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.4622\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.4572\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 29s 161ms/step - loss: 0.4539\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.4466\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 31s 164ms/step - loss: 0.4405\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.4382\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 30s 162ms/step - loss: 0.4357\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.4315\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 30s 161ms/step - loss: 0.4273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rebuilding the model\n",
        "After training the model we need to rebuild it with different number of batch size. We had already build it with batch size = 64 which means that we had to pass a 64 entries at a time. Now we'll rebuild it with batch_size =1 and load the weights from the training. "
      ],
      "metadata": {
        "id": "nDyzE37qwtXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "mCI0SOOXZom8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) # load the points from the latest checkpoints\n",
        "model.build(tf.TensorShape([1, None])) # batch size = 1 with unknown length"
      ],
      "metadata": {
        "id": "aipR5vMEZqUG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code to get a vertain checkpoint\n",
        "# checkpoint_num = 10\n",
        "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
        "# model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "axoCTkpgZqv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Text"
      ],
      "metadata": {
        "id": "QFFD8cHfZswM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)  \n",
        "  # we are expanding the dimensions because the model is expecting a input with 1 dimension\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # remove the batch dimension\n",
        "      # the predictions will be in neasted lists\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # convert the text back to string\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "iDgfpIw-ZuBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "id": "Yn_qSzJUZwZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}