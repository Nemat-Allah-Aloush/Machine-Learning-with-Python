{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning"
      ],
      "metadata": {
        "id": "_k9bCJMu5Hlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea of Reinforcement learning is to let the model explore the enviroment itself, it is similar to when a human tries to learn a game by playing it, he makes mistakes along the way and tries to remeber them and not repeat them.\n",
        "\n",
        "There different types of Reinforcement learning, in this example we'll talk about ***Q-learning***."
      ],
      "metadata": {
        "id": "w0B3GTKpJ6Fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terminology\n",
        "Before we dive into explaining reinforcement learning we need to define a few key peices of terminology.\n",
        "\n",
        "**Enviornemt** In reinforcement learning tasks we have a notion of the enviornment. This is what our agent will explore. An example of an enviornment in the case of training an AI to play say a game of mario would be the level we are training the agent on.\n",
        "\n",
        "**Agent** an agent is an entity that is exploring the enviornment. Our agent will interact and take different actions within the enviornment. In our mario example the mario character within the game would be our agent.\n",
        "\n",
        "**State** always our agent will be in what we call a state. The state simply tells us about the status of the agent. The most common example of a state is the location of the agent within the enviornment. Moving locations would change the agents state.\n",
        "\n",
        "**Action** any interaction between the agent and enviornment would be considered an action. For example, moving to the left or jumping would be an action. An action may or may not change the current state of the agent. In fact, the act of doing nothing is an action as well! The action of say not pressing a key if we are using our mario example.\n",
        "\n",
        "**Reward** every action that our agent takes will result in a reward of some magnitude (positive or negative). The goal of our agent will be to maximize its reward in an enviornment. Sometimes the reward will be clear, for example if an agent performs an action which increases their score in the enviornment we could say they've recieved a positive reward. If the agent were to perform an action which results in them losing score or possibly dying in the enviornment then they would recieve a negative reward.\n",
        "\n",
        "The most important part of reinforcement learning is determing how to reward the agent. After all, the goal of the agent is to maximize its rewards. This means we should reward the agent appropiatly such that it reaches the desired goal."
      ],
      "metadata": {
        "id": "WVeHz_iILFyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qlearning\n"
      ],
      "metadata": {
        "id": "KsaKDKYhNUYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea in this technique is to learn a matrix of action-reward values, this matix is refered to as Q-Table or Q-matrix. The rows are the states and the columns are the actions, the  cells represent the rewards we'll gain in case we have performed the action in the corresponding column at the state of the corresponding row.\n",
        "\n",
        "The goal is to learn from the table the actions that will give us the maximum rewards. However if the agent follow only the q-table it can get stuck in a local maximum, to avoid that the model can randomly choose actions in the begining in order to learn the enviroment more freely. "
      ],
      "metadata": {
        "id": "Z5LW3IAwNYHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to fill the values in the q-table ? \n",
        "There are 2 parameters to consider:\n",
        "* learning rate, this vlaue tells us how much we can change the values in the q-table in each observation.\n",
        "* Discount Factor, try to define the balance between finding the maximum value in our the current state and finding the reward from the next state. The higher the value is the more we're going to look for the future, and vice versa.\n",
        "\n",
        "The idea is try to look forward, not only to look what reward can a get from taking an action in a certain state, but to look forward and look what reward will I gain when I move to the next state after performing this action."
      ],
      "metadata": {
        "id": "v4YS10LwWSin"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W2oj-_me49DP"
      },
      "outputs": [],
      "source": [
        "import gym   # Importing AI gym to to train a reinforcement learning model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0')  # we are going to use the FrozenLake enviornment\n"
      ],
      "metadata": {
        "id": "YIk10jy95fKx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI gym has an observation space and action space for the enviroment.\n"
      ],
      "metadata": {
        "id": "MD-Ivh9oY_eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space.n)   # get number of states\n",
        "print(env.action_space.n)   # get the number of actions that we can do at any different state."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5TjEu9Q5glL",
        "outputId": "71b38d77-86fe-4e15-fbbd-f32d057ef6f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()  # reset enviornment to default state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YObG_t0w5hh_",
        "outputId": "978bf63e-6e56-46b3-f44c-00eaf972c277"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action = env.action_space.sample()  # we can pick a random action "
      ],
      "metadata": {
        "id": "jLq2x_qG5ikZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_state, reward, done, info = env.step(action)  \n",
        "# take action and perform it on the enviroment,\n",
        "# it returns information about the action : new state, the reard we got, lose or win ? ,info "
      ],
      "metadata": {
        "id": "77MoZklS5je_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()   # render the GUI for the enviornment to show the enviroment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdwoaCfK5kcq",
        "outputId": "5351c4a2-a1d0-46fd-d28e-fa2c08d5341a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start, Frozen, Halls,Goal."
      ],
      "metadata": {
        "id": "eZfyWGYqaVfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the model \n",
        "The task is to navigate the enviroment, got from the Start to the Goal."
      ],
      "metadata": {
        "id": "3i4OvperapTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "cUogkb605l3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS = env.action_space.n"
      ],
      "metadata": {
        "id": "3JhJ5le-5nDI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the q-table\n",
        "Intialize the Q-table with zero values, because at the begining the agent does not know anything about the enviroment. "
      ],
      "metadata": {
        "id": "Rk6uOk4ebFNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = np.zeros((STATES, ACTIONS))  # create a matrix with all 0 values \n",
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "set-KmzB5oSr",
        "outputId": "4c0b5fe1-8231-4a89-b0ab-d23a130c666d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Constants"
      ],
      "metadata": {
        "id": "fJE1dDti5pSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 2000 # how many times to run the enviornment from the beginning\n",
        "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
        "\n",
        "LEARNING_RATE = 0.81  # learning rate, the higher the learning rate is the faster it learns.\n",
        "GAMMA = 0.96"
      ],
      "metadata": {
        "id": "oY6VN6u35q4Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Picking an action\n",
        "\n",
        "\n",
        "```\n",
        "epsilon = 0.9  # start with a 90% chance of picking a random action\n",
        "\n",
        "# code to pick action\n",
        "if np.random.uniform(0, 1) < epsilon:  # we will check if a randomly selected value is less than epsilon.\n",
        "    action = env.action_space.sample()  # take random action\n",
        "else:\n",
        "    action = np.argmax(Q[state, :])  # use Q table to pick best action based on current values\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FQZbiMpMJfvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updating Q values\n",
        "\n",
        "```\n",
        "Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state, action])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ReIFnmIoJi6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it Together"
      ],
      "metadata": {
        "id": "Q-GoysIiJmmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "STATES = env.observation_space.n\n",
        "ACTIONS = env.action_space.n\n",
        "\n",
        "Q = np.zeros((STATES, ACTIONS))\n",
        "\n",
        "EPISODES = 1500 # how many times to run the enviornment from the beginning\n",
        "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
        "\n",
        "LEARNING_RATE = 0.81  # learning rate\n",
        "GAMMA = 0.96\n",
        "\n",
        "RENDER = False # if you want to see training set to true\n",
        "\n",
        "epsilon = 0.9\n"
      ],
      "metadata": {
        "id": "h6_9k7eSJoDo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = [] # save all the rewards that we will get \n",
        "for episode in range(EPISODES):\n",
        "\n",
        "  state = env.reset() # reset to get the starting state\n",
        "  for _ in range(MAX_STEPS): # explore the enviroment up to the max step\n",
        "    \n",
        "    if RENDER:\n",
        "      env.render()\n",
        "\n",
        "    # Take an action\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample()  \n",
        "    else:\n",
        "      action = np.argmax(Q[state, :])\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "    # Updating the q-table\n",
        "    Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
        "\n",
        "    #Updating the current state variable\n",
        "    state = next_state\n",
        "\n",
        "    # If the agent die, lost, ...\n",
        "    if done: \n",
        "      rewards.append(reward) # Get the reward, 0 if died, and 1 every time we move to a valid spot\n",
        "      epsilon -= 0.001 # reduce the epsilon if we lost, reduce it slowly to \n",
        "      break  # reached goal\n",
        "\n",
        "print(Q)\n",
        "print(f\"Average reward: {sum(rewards)/len(rewards)}:\")\n",
        "# and now we can see our Q values!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtG8x0NjJpep",
        "outputId": "30cb4e5f-9d86-4be9-842b-b5d2d4f1b024"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.59407054e-01 3.59579356e-02 3.89163065e-02 3.97550718e-02]\n",
            " [3.00132762e-03 1.00184038e-02 1.69785514e-02 4.20600522e-01]\n",
            " [9.97465579e-03 1.10057532e-02 1.73192325e-02 3.41453678e-01]\n",
            " [1.30503523e-02 9.58107395e-03 7.74275009e-03 1.13654005e-01]\n",
            " [5.02889575e-01 2.96568914e-03 6.02689313e-03 8.32417618e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.29978236e-04 5.39622019e-04 3.15695739e-01 7.61175600e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.92989730e-02 2.30439433e-03 1.43544438e-02 3.77200877e-01]\n",
            " [8.60834521e-03 2.52653313e-01 3.00964173e-02 1.66549363e-02]\n",
            " [8.12261462e-01 1.04273512e-03 1.46362176e-03 4.83486964e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.45015131e-02 4.00618469e-02 4.38079124e-01 2.55407815e-02]\n",
            " [1.10384373e-01 9.85800901e-01 1.29164778e-01 1.37423383e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Average reward: 0.33:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can plot the training progress and see how the agent improved\n",
        "# we are plotting the average reward over a hundred step from the begining to the end\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_average(values):\n",
        "  return sum(values)/len(values)\n",
        "\n",
        "avg_rewards = []\n",
        "for i in range(0, len(rewards), 100):\n",
        "  avg_rewards.append(get_average(rewards[i:i+100])) \n",
        "\n",
        "plt.plot(avg_rewards)\n",
        "plt.ylabel('average reward')\n",
        "plt.xlabel('episodes (100\\'s)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "dNFCRJzMJqjQ",
        "outputId": "80fe7c94-44d6-49d3-9f7e-f6a8920977ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}